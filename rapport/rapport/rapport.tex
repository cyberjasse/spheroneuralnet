\documentclass[12pt,a4paper,oneside, titlepage]{article}
\usepackage[left=2cm,right=2cm,top=2cm,bottom=2cm]{geometry}
\usepackage[pdftex]{graphicx}
\usepackage[final]{pdfpages}
\usepackage[frenchb]{babel}
\usepackage{xspace}
\usepackage{lmodern}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\newcommand{\tabitem}{~~\llap{\textbullet}~~}
\begin{document}
\input{../titlepage}
\input{../commands}
\spherotitle{Rapport}{Année 2016-2017}
\tableofcontents
\newpage
\input{introduction}

\newpage
\section{Théorie}
\subsection{Modèles de réseaux de neurones artificiels}
\terminologie
Tout d'abord, analysons les différents modèles de réseau de neurones artificiels éxistants afin de pouvoir sélectionner celui qui convient le mieux à notre application.
Les plus connus d'entre eux sont le perceptron multi-couche et la fonction à base radiale, tous deux non récurrents.
Ensuite nous verrons les réseaux de neurones récurrents qui approximent des fonctions qui peuvent dépendre de toutes les entrées précédentes.
Les terminologies utilisées proviennent de la littérature anglophone et tous ceux rencontrés sont répertoriés à la table \ref{terminologie}.
\newcommand{\ssstitle}[1]{\begin{center}\large\underline{#1}\normalsize\end{center}}
\input{mlp}
\input{rbf}
%\input{cnn}
\input{rnn}
%\input{cmac}

\input{surgeneralisation}

\input{architecture}

\newpage
\section{Implémentation et application}

\input{design}

\subsection{Choix du réseau}
%TODO Tableau de comparaison
Nous n'avons pas besoin d'un réseau de neurones récurrent.
Les commandes à appliquer ne dépendent pas de la vitesse, position et autres données fournies aux étapes précédentes.
Le réseau le mieux adapté pour notre problème est un réseau de neurones à base radial.
En effet, les \rbf sont moins sensibles au bruit que les \mlp \cite{adversarial,Gauthier}.%gauthier p 39,40
Car dans la couche cachée, chaque neurone envoit une sortie indiquant la proximité entre le vecteur d'entrée et son prototype.
Donc typiquement, la contribution à l'erreur est presque entièrement celle du neurone ayant son prototype le plus proche de l'entrée.
Les autres neurones cachés auront une modification insignifiante de l'écart-type et du prototype.\\

Un des inconvénients des \rbf est que le nombre de neurones cachés croît avec la dimension et la taille du vecteur d'entrée.
Ceci ne pose pas de problème dans notre cas car le nombre de dimensions est faible.

\subsection{Choix de la méthode}
Vu toutes les inconnues mécaniques du mouvement de la Sphero, il s'avère difficile d'établir une équation permettant de prédire de manière précise l'état suivant en fonction des commandes exécutées.
Nous préfèrons donc nous passer d'un modèle analytique.
Dans ce cas, nous ne pouvons pas utiliser la méthode de l'apprentissage spécialisé.
De plus, télécommander une Sphero pour suivre une trajectoire est très difficile pour un humain.
Utiliser un apprentissage par reproduction de contrôleur n'est donc pas envisageable.\\
Nous pouvons obtenir un maître distant à chaque étape et donc nous n'avons pas besoin de l'architecture de Nguyen et Widrow.\\
Le problème de convergence vers une solution triviale, impliqué dans le cas d'un apprentissage indirect (section \ref{sec:appindirect}) exclu l'utilisation de cette méthode dans le cadre de ce projet.\\

Il nous reste deux méthodes pour lesquelles l'adaptabilité est sacrifiée: l'apprentissage en deux phases et apprentissage utilisant un modèle différentiable.
Les deux sont envisageables. Les tests seront effectués toujours sur le même sol horizontal.\\

La méthode avec modèle différentiable apportera un nouveau facteur influençant la réussite du projet: la fiabilité du modèle.
Mais puisque le sytème ne change pas, lorsqu'on arrive à obtenir un modèle fiable, nous pouvons effectuer un apprentissage en-ligne qui permettra à la Sphero de parfaire le suivi d'une trajectoire si cette trajectoire est ammenée à être répétée.
Nous utiliserons dans un premier temps la méthode en deux phases pour obtenir un ensemble de paires cible/commande où la précision dépend seulement des capteurs et pas de la fiabilité d'un modèle.

\subsection{Les API}
Certaines API permettant l'envoi de commande et réception de message provenant de la Sphero sont disponibles.
Les APIs officiels sont disponibles en: \textbf{Objective-C}, \textbf{Swift}, \textbf{Android}\cite{SDKofficiels}.\\
Il existe également des APIs créés par la communauté disponibles en\cite{gosphero}: \textbf{C\#}, \textbf{JavaScript}, \textbf{Ruby}, \textbf{Python}\cite{pythonAPI}, \textbf{Arduino}, \textbf{C++}\cite{cppAPI}.\\
L'API choisie est l'API \textbf{C++} pour le langage de programmation connu et performant exécutable sur PC.

Le langage \textbf{Qt} sera utilisé pour créer les interfaces graphiques.
Cela nous permet de créer facilement des interfaces munies de boutons, menus et fenêtre afin de tracer la trajectoire.
De plus, l'implémentation du réseau de neurones pourra être directement utilisée dans un script \textbf{Qt}.

\subsection{Diagramme de classe}
Afin d'expliquer la structure de l'implémentation du réseau de neurone, nous allons tout d'abord expliquer pourquoi l'implémentation d'un neurone est indépendant de sa position et du modèle du réseau de neurones.
Et grâce à cette propriété, nous pouvons donc connecter n'importe quelle couche de neurones avec une autre.
Nous pourrions donc facilement passer à une méthode d'apprentissage par modèle différentiable (Section \ref{sec:appmodele}) si cela s'avère nécessaire.
De plus, l'implémentation d'un nouveau modèle non récursif comme le \mlp nécésitera juste l'implémentation des nouveaux neurones.
En résumé, l'implémentation d'un réseau \rbf se fera à partir d'une implémentation de réseau de neurones non récursif modulaire.\\

L'implémentation d'un neurone se fait indépendamment de sa position dans le réseau et des autres neurones.
En effet, deux fonctions sont nécessaires pour l'implémentation d'un neurone:
\begin{enumerate}
 \item Une fonction qui permet de générer la sortie à partir de ses entrées (compute).
 \item Une fonction permettant l'apprentissage par rétropropagation (errorContribution).
\end{enumerate}
La fonction qui génère la sortie n'a besoin que des sorties des neurones de la couche précédente.
La fonction qui permet l'apprentissage n'a besoin que de la sortie générée précédemment (cela peut être gardé en mémoire) et la \emph{contribution à l'erreur} des neurones de la couche suivante.
Le terme de contribution à l'erreur fait réference au terme $\delta$ dans la formule de modification des poids d'un \mlp (Section \ref{sec:appmlp}).
En effet, pour tous les différents neurones que nous avons vu dans ce rapport, soit $p$ un paramètre à modifier pour l'apprentissage du neurone $i$.
Soit $f_i$ la fonction appliquée aux entrées du neurone $i$ afin de générer la sortie.
$p$ doit être modifié selon la formule suivante:
\[\Delta p_i = -\eta \partiel{Q}{p_i}\]
Et afin de calculer la valeur de $\partiel{Q}{p_i}$, deux facteurs sont à connaître: $\partiel{Q}{f_i}$ et $\partiel{f_i}{p_i}$.
Pour connaître le facteur $\partiel{f_i}{p_i}$, toutes les données dont nous avons besoin se trouvent dans l'implémentation du neurone $i$ si nous y enregistrons sa sortie précédente.
Tandis que pour le facteur $\partiel{Q}{f_i}$,
si le neurone $i$ est un neurone de sortie, alors $\partiel{Q}{f_i}$ est connu, il s'agit de la dérivée partielle de l'erreur entre $f_i$ et sa valeur atendue.
Sinon $$\partiel{Q}{f_i} = \sum_{k \in \text{couche suivate}}\partiel{Q}{f_k}\partiel{f_k}{f_i}$$
Et donc $\partiel{Q}{f_i}$ peut être calculé à partir d'une implémentation dans les neurones de la couche suivante.

\begin{figure}
 \centering
 \includegraphics[width=\textwidth]{../../uml/neurondiag.png}
 \caption{Diagramme de classe du réseaux de neurones. \footnotesize Généré par \href{http://plantuml.com/class-diagram}{plantUML}}
 \label{fig:diagclasse}
\end{figure}
Dans le diagramme de classe, Figure \ref{fig:diagclasse}, dans la déclaration de la fonction \texttt{errorContribution()}, \texttt{errorContribution} est le facteur $\partiel{Q}{f_i}$.
Le tableau retourné est stocké dans \texttt{thisContribution}.
Pour chaque entrée $x_j$ du neurone $i$, la $j$\textsuperscript{ième} valeur du tableau retourné est $\partiel{Q}{f_i}\partiel{f_i}{x_j}$.
Ces valeurs sont utilisées pour effectuer la rétropropagation de la couche précédente.

\newpage
%\section{Validation}

\subsection*{Remerciements}
\noindent Je remercie Monsieur Pierre \textsc{Hauweele} de m'avoir proposé un projet qui est exactement dans le domaine que je voulais, de son aide et de la direction de mon projet.\\

\noindent Je remercie Monsieur Hadrien Mélot pour la direction de mon projet et de l'aide qu'il m'a apporté.\\

\noindent Je remercie Monsieur Tom Mens d'être rapporteur de mon projet et de son aide.\\

\bibliographystyle{unsrt-fr}%unsrt-fr pour reference fr par num. plainnat-fr ou frplain Pour reference fr par auteur et annee
\bibliography{../bibli}
\appendix

\input{backpropamlp}
\input{backproparbf}

\end{document}
