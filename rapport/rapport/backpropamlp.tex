\section{Équations de rétropropagation d'un MLP}\label{sec:eqmlp}
La modification à apporter au poids $j$ du neurone $i$ vaut
\begin{equation}\label{eq:Delta}
 \Delta W_{ij} = -\eta\partiel{Q}{W_{ij}}
\end{equation}
\begin{equation}\label{eq:gradient}
 \partiel{Q}{W_{ij}} = \partiel{Q}{\phi_i} \partiel{\phi_i}{v_i} \partiel{v_i}{W_{ij}}
\end{equation}
Et on a posé
\begin{equation}\label{eq:deltai}
 \delta_i = \partiel{Q}{\phi_i} \partiel{\phi_i}{v_i}
\end{equation}
En reprenant \eqref{eq:gradient}, nous avons maintenant que
\begin{equation}\label{eq:graddelta}
 \partiel{Q}{W_{ij}} = \delta_i \partiel{v_i}{W_{ij}}
\end{equation}
Nous allons déterminer la valeur de $\delta_i$ et de $\partiel{v_i}{W_{ij}}$.
Commençons par déterminer $\partiel{v_i}{W_{ij}}$.
Soit $x_{ij}$ la $j$\textsuperscript{ième} entrée du neurone $i$,
\begin{equation}\label{eq:gradsum}
 \begin{split}
  \partiel{v_i}{W_{ij}} & = \partiel{(W_{i0}x_{i0})}{W_{ij}} + \partiel{(W_{i1}x_{i1})}{W_{ij}} + ... + \partiel{(W_{ij}x_{ij})}{W_{ij}} + ... + \partiel{(W_{im}x_{im})}{W_{ij}}\\
  ~ & = x_{ij}
  \end{split}
\end{equation}
Ensuite, déterminons la valeur de $\delta_i$ selon deux cas: si $i$ est un neurone de sortie et si $i$ est un neurone caché.\\

\textbf{Si le neurone $i$ est un neurone de sortie,}
\begin{equation}\label{eq:sortie1}
 \begin{split}
  \partiel{Q}{\phi_i} & = \frac{1}{2} \left( \partiel{(\phi_0 - s_0)^2}{\phi_i} + \partiel{(\phi_1 - s_1)^2}{\phi_i} + ... + \partiel{(\phi_i - s_i)^2}{\phi_i} + ... + \partiel{(\phi_l - s_l)^2}{\phi_i} \right)\\
  ~ & = \frac{1}{2}2(\phi_i - s_i)\partiel{(\phi_i - s_i)}{\phi_i}\\
  ~ & = (\phi_i - s_i)
 \end{split}
\end{equation}
Soit $\phi'$ la dérivée de $\phi$,
\begin{equation}\label{eq:sortie2}
 \partiel{\phi_i}{v_i} = \phi'(v_i)
\end{equation}
Par \eqref{eq:sortie1} et \eqref{eq:sortie2}, on a
\[\delta_i = (\phi_i - s_i)\phi'(v_i)\]\\

\textbf{Si le neurone $i$ est un neurone caché,}
alors soit $n$ le nombre de neurones de la couche suivante (plus proche des neurones de sorties), soit $k$ un neurone de la couche suivante, on sait que $Q$ est fonction composée,
dépendant de $\phi_k$, lui-même dépendant de $v_k$, dépendant lui-même de $\phi_i$.
C'est pour cela qu'en appliquant le théorème de dérivée des fonctions composées,
\begin{equation}\label{eq:cache1beforebefore}
 \partiel{Q}{\phi_i} = \sum_{k=1}^{n} \partiel{Q}{\phi_k} \partiel{\phi_k}{v_k} \partiel{v_k}{\phi_i}
\end{equation}
Si on subsitue \eqref{eq:deltai} dans \eqref{eq:cache1beforebefore},
\begin{equation}\label{eq:cache1before}
 \partiel{Q}{\phi_i} = \sum_{k=1}^{n} \delta_k \partiel{v_k}{\phi_i}
\end{equation}
Or $\phi_i$ est l'entrée du neurone $k$ recevant la sortie du neurone $i$. Posons $W_{ki}$ le poids que $k$ attribue à $\phi_i$.
On peut alors continuer à développer \eqref{eq:cache1before} comme suit:
\begin{equation}\label{eq:cache1}
 \partiel{v_k}{\phi_i} = \partiel{W_{ki}\phi_i}{\phi_i} = W_{ki}
\end{equation}
Pour la valeur de $\partiel{\phi_i}{v_i}$, nous pouvons reprendre \eqref{eq:sortie2}.
Et du coup, \[\delta_i = \phi_i'(v_i) \sum_{k=1}^{n} \delta_k W_{ki}\]

Nous connaissons maitenant la modification à appliquer sur $W_{ij}$. Pour résumé, subsituons $\delta_i$ et \eqref{eq:gradsum} à \eqref{eq:gradient},
\begin{equation}\label{eq:subgradient}
 \partiel{Q}{W_{ij}} = \delta_i x_{ij}
\end{equation}
Qu'on subsitue à \eqref{eq:Delta} et nous avons enfin
\begin{equation}\label{eq:mlpretro}
 \Delta W_{ij} = -\eta \delta_i x_{ij} \text{~où~}\left\{
  \begin{array}{lll}
   \delta_i & = (\phi_i - s_i)\phi'(v_i) & \text{Si~} i \text{~est un neurone de sortie}\\
   \delta_i & = \phi_i'(v_i) \sum_{k=1}^{n} \delta_k W_{ki} & \text{Si~} i \text{~est un neurone caché}
  \end{array}
 \right.
\end{equation}
